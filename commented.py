from __future__ import absolute_import, division, print_function

import os
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow.contrib.eager as tfe

train_dataset_fp = "/Users/tomokatsuiguchi/.keras/datasets/iris_training.csv"

tf.enable_eager_execution()

"""
データの取得/整形: Dataset
"""
def parse_csv(line):
  example_defaults = [[0.], [0.], [0.], [0.], [0]]  # float,float,float,float,int
  parsed_line = tf.decode_csv(line, example_defaults) # Tensorのリストを返却 @see: https://www.tensorflow.org/api_docs/python/tf/decode_csv
  # TensorからTensorを作る。行列の形式を変える。(4,) => 4x1 = [1,2,3,4]
  # ex: [1, 2, 3, 4, 5, 6, 7, 8, 9] 
  #    (3,3) =>  [[1,2,3], [4,5,6], [7,8,9]]
  #    (-1)  => flatten (-1を入れるとフラット)
  features = tf.reshape(parsed_line[:-1], shape=(4,))
  label = tf.reshape(parsed_line[-1], shape=()) # () => scalarになる(1x1)
  return features, label # タプル: 特徴<Tensor>, ラベル<Tensor>


train_dataset = (tf.data
                .TextLineDataset(train_dataset_fp) # CSVから 'Dataset' オブジェクトを返す
                .skip(1)             # 最初の行をスキップ
                .map(parse_csv)     # マップ
                .shuffle(buffer_size=1000)  # シャッフル 偏りがなくなるようにしておく(データサイズよりも大きくする)
                .batch(32))  # バッチサイズ毎に分割(効率的にするため): [[1,2,..32], [2,3,4,..33], [3,4,5,..34] ...]

# １行目だけ見てみる
features, label = tfe.Iterator(train_dataset).next()
print("example features:", features[0]) # tf.Tensor([5.9 3.2 4.8 1.8], shape=(4,), dtype=float32)
print("example label:", label[0])       # tf.Tensor(1, shape=(), dtype=int32)

"""
モデルの生成: keras
"""
# ノード数が10の2層のモデル
# アウトプットが３つ(labelの推測)
# input_shape(必須): 入力の形式
model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation="relu", input_shape=(4,)),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(3)
])

"""
モデルの学習関数の定義
"""
# 損失関数
def loss(model, x, y):
  y_ = model(x) # y_: モデルが算出した結果
  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_) # 交差エントロピー: yとy_の誤差を計算。誤差Tensorを返す
# 微分
def grad(model, inputs, targets):
    # @see: https://www.tensorflow.org/api_docs/python/tf/contrib/eager/GradientTape
  with tfe.GradientTape() as tape: # 微分計算の処理を記録する。出力値を自動的にWatchする。 gradient()が呼ばれるとすぐに、リソースがリリースされる。
    loss_value = loss(model, inputs, targets)
  return tape.gradient(loss_value, model.variables) # tapeによってトレースされた情報を使って微分する。variablesによって、loss_valueの誤差がでている。という関係性。

# 確率的勾配降下法
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) # ステップ幅

"""
学習ループ
"""

train_loss_results = []
train_accuracy_results = []

num_epochs = 201 #ループ回数: 回数を増やせば増やすほどモデルが良くなるとは限らない。経験と実験から値を調整する必要がある。

for epoch in range(num_epochs):
  epoch_loss_avg = tfe.metrics.Mean()
  epoch_accuracy = tfe.metrics.Accuracy()

  # 学習ループ: 32のbatchを使う
  for x, y in tfe.Iterator(train_dataset): # タプル: 特徴<Tensor>, ラベル<Tensor>
    # 最適化
    grads = grad(model, x, y) # モデルの実行 -> 損失関数計算 -> 微分
    optimizer.apply_gradients(zip(grads, model.variables), # List of (gradient, variable) pairs
                              global_step=tf.train.get_or_create_global_step()) # global_stepを増加する

    # Track progress
    epoch_loss_avg(loss(model, x, y))  # add current batch loss
    # compare predicted label to actual label
    # model(x):   tf.Tensor(
    #            [[ 5.420086    0.9100239  -5.528735  ]
    #             [-2.0589843   2.9003973  -0.82532704]
    #             [-5.399555    1.8863136   3.4920776 ]
    #             [ 4.9513235   0.787956   -5.0430694 ]
    #             [-3.6721003   2.0134077   1.7159663 ]
    #             [-2.0693643   2.2782702  -0.20707467]
    #             [ 5.2438602   0.8102783  -5.367257  ]
    #             [ 4.567486    0.8200134  -4.808399  ]
    #             [ 5.496583    0.8844168  -5.672711  ]
    #             [-4.0997286   1.4131113   2.760695  ]
    #             [ 5.961009    1.0276215  -6.0796313 ]
    #             [-5.68324     0.64157975  5.1793594 ]
    #             [-5.6625757   1.296332    4.4495645 ]
    #             [-5.88729     0.74797237  5.281938  ]
    #             [ 4.642065    0.809026   -4.842775  ]
    #             [-3.9742892   1.4147228   2.6388628 ]
    #             [ 5.4863844   0.97600615 -5.763962  ]
    #             [-2.4665048   2.1782403   0.28724855]
    #             [ 4.9989185   0.80582166 -5.065466  ]
    #             [-2.6029036   2.4894445   0.11991467]
    #             [ 5.2738523   0.9156034  -5.3425183 ]
    #             [-4.9306974   1.0828193   3.9621997 ]
    #             [-2.3972585   2.9849186  -0.6071105 ]
    #             [ 4.6781874   0.7448106  -4.787163  ]], shape=(24, 3), dtype=float32)
    # tf.argmax:  tf.Tensor([2 1 2 1 2 1 0 2 1 2 1 2 0 0 2 1 0 0 0 0 2 0 1 0], shape=(24,), dtype=int32): 最大値のindexを返す
    # y:          tf.Tensor([1 1 1 1 2 1 1 1 0 1 2 2 2 0 2 2 2 1 0 2 2 0 1 1], shape=(24,), dtype=int32)
    epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)

  # end epoch
  train_loss_results.append(epoch_loss_avg.result())
  train_accuracy_results.append(epoch_accuracy.result())

  # 50回ごとに結果を表示
  if epoch % 50 == 0:
    print("Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch,
                                                                epoch_loss_avg.result(),
                                                                epoch_accuracy.result()))
  if epoch % 200 == 0:
    # print("model(x): ", model(x))
    # print("model(x): ", tf.argmax(model(x), axis=1, output_type=tf.int32))
    # print("x ", x)


    """ サンプルデータ(変数の中身が取りうる値を見てみて理解を深める)

    - x: 特徴<Tensor>

tf.Tensor(
[[4.6 3.4 1.4 0.3]
 [4.9 3.1 1.5 0.1]
 [6.7 3.1 5.6 2.4]
 [5.7 2.9 4.2 1.3]
 [7.7 3.  6.1 2.3]
 [7.3 2.9 6.3 1.8]
 [5.  3.4 1.5 0.2]
 [6.  3.  4.8 1.8]
 [6.9 3.1 4.9 1.5]
 [5.2 3.5 1.5 0.2]
 [6.5 3.  5.2 2. ]
 [6.7 3.1 4.4 1.4]
 [5.4 3.7 1.5 0.2]
 [4.5 2.3 1.3 0.3]
 [6.1 3.  4.9 1.8]
 [6.1 2.8 4.7 1.2]
 [4.7 3.2 1.6 0.2]
 [6.  2.9 4.5 1.5]
 [6.2 2.8 4.8 1.8]
 [5.7 2.8 4.5 1.3]
 [5.8 2.7 5.1 1.9]
 [6.3 3.3 4.7 1.6]
 [4.9 3.  1.4 0.2]
 [5.5 2.4 3.8 1.1]], shape=(24, 4), dtype=float32)

    - y: ラベル<Tensor>

tf.Tensor([1 1 1 1 2 1 1 1 0 1 2 2 2 0 2 2 2 1 0 2 2 0 1 1], shape=(24,), dtype=int32)


    - model(x): モデルに特徴をいれて計算した結果<Tensor>

tf.Tensor(
[[-3.5308294   3.0940378   4.5754576 ]
 [-3.490197    2.91866     4.291092  ]
 [-1.9586891   3.8336287   2.674446  ]
 [ 9.261741    4.251009   -8.059412  ]
 [-3.2579606   4.6086473   4.2514415 ]
 [ 8.34307     4.211892   -7.347712  ]
 [ 8.610467    4.473446   -7.5209966 ]
 [-0.8593468   3.5201106   1.5182462 ]
 [-1.4212502   4.1281996   2.1122065 ]
 [-4.2931814   2.4316516   5.108483  ]
 [-4.3982654   3.1208837   5.0170245 ]
 [-0.4347648   4.206646    1.0881323 ]
 [-0.08312028  4.399291    0.7130943 ]
 [ 0.04532129  4.4718056   0.58970326]
 [ 8.543274    4.2387414  -7.531213  ]
 [ 1.2138404   4.2417545  -0.43575174]
 [ 0.7633007   3.8962595  -0.2006746 ]
 [ 0.77899605  3.9489124  -0.20968467]
 [ 8.266623    4.073799   -7.2308335 ]
 [-3.691562    3.3593566   4.516098  ]
 [ 0.03922255  4.2867413   0.53582233]
 [ 8.748198    4.1545587  -7.6492896 ]
 [-3.3934534   3.1820943   4.1146455 ]
 [ 9.936032    4.264922   -8.562887  ]], shape=(24, 3), dtype=float32)

    - grads: 微分した結果
4*10, 10, 10*10, 10, 10*3, 3

[
    <tf.Tensor: id=133570, shape=(4, 10), dtype=float32, numpy=
array([[-0.00082401,  0.2190536 , -0.03594472,  0.04177892,  0.        ,
         0.        , -0.07532994, -0.01379006, -0.07807101,  0.07629745],
       [-0.00027824,  0.08622543, -0.013728  ,  0.01686772,  0.        ,
         0.        , -0.03177056, -0.00298843, -0.03293865,  0.03501828],
       [-0.0007384 ,  0.19104546, -0.0319116 ,  0.03553322,  0.        ,
         0.        , -0.0614994 , -0.01795679, -0.06405558,  0.0559576 ],
       [-0.00024613,  0.08267819, -0.01323871,  0.01513698,  0.        ,
         0.        , -0.02563576, -0.00870094, -0.02732233,  0.02300887]],
      dtype=float32)>, 
      
<tf.Tensor: id=133568, shape=(10,), dtype=float32, numpy=
array([-0.00010701,  0.02347373, -0.0040928 ,  0.0048504 ,  0.        ,
        0.        , -0.00919129, -0.00094096, -0.00913462,  0.00938595],
      dtype=float32)>, 

<tf.Tensor: id=133566, shape=(10, 10), dtype=float32, numpy=
array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,
        -1.3193570e-05,  0.0000000e+00,  0.0000000e+00,  4.6324294e-05,
        -9.0234083e-05, -6.5701832e-05],
       [-1.5407528e-02, -9.8350914e-03,  0.0000000e+00,  0.0000000e+00,
         1.4067246e-02, -4.0677818e-03,  4.1922922e-03,  4.2251484e-03,
        -1.3389917e-02, -8.9373747e-03],
       [-6.5840450e-03,  3.6308132e-02,  0.0000000e+00,  0.0000000e+00,
        -1.2189704e-02, -4.1825636e-03,  1.1721412e-01,  6.1291598e-02,
        -1.3005990e-01, -9.0643026e-02],
       [-3.2204920e-03,  4.4130147e-03,  0.0000000e+00,  0.0000000e+00,
         4.0743209e-05, -1.2403959e-03,  1.9223630e-02,  1.0324259e-02,
        -2.2533456e-02, -1.5637644e-02],
       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,
         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,
         0.0000000e+00,  0.0000000e+00],
       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,
         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,
         0.0000000e+00,  0.0000000e+00],
       [ 1.6647439e-03,  1.0399650e-02,  0.0000000e+00,  0.0000000e+00,
        -5.5360454e-03, -4.7879198e-06,  2.5331637e-02,  1.2747755e-02,
        -2.5793921e-02, -1.8173821e-02],
       [ 3.5975701e-03,  1.6789794e-02,  0.0000000e+00,  0.0000000e+00,
        -1.0145297e-02, -1.1956392e-04,  4.2221837e-02,  2.1520248e-02,
        -4.4488046e-02, -3.1037860e-02],
       [ 3.0266887e-03,  1.4041780e-02,  0.0000000e+00,  0.0000000e+00,
        -8.3911838e-03, -6.6467569e-06,  3.4619194e-02,  1.7558880e-02,
        -3.5996851e-02, -2.5166040e-02],
       [-1.4435237e-03,  9.6421223e-03,  0.0000000e+00,  0.0000000e+00,
        -3.4876736e-03, -1.0555749e-03,  3.1064408e-02,  1.6293805e-02,
        -3.4606624e-02, -2.4152640e-02]], dtype=float32)>, 

<tf.Tensor: id=133564, shape=(10,), dtype=float32, numpy=
array([-0.00188865,  0.00170763,  0.        ,  0.        ,  0.00039438,
       -0.00088056,  0.00943705,  0.0051989 , -0.01222188, -0.00837619],
      dtype=float32)>, 
      
<tf.Tensor: id=133562, shape=(10, 3), dtype=float32, numpy=
array([[-5.7749273e-03,  5.6214565e-03,  1.5346041e-04],
       [-1.4901528e-02,  2.0098627e-02, -5.1971283e-03],
       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],
       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],
       [ 5.8905231e-03,  1.2162252e-01, -1.2751311e-01],
       [-1.1317758e-03,  1.1313525e-03,  4.2086546e-07],
       [-1.8766398e-02,  2.6368424e-02, -7.6020663e-03],
       [-9.5083527e-03,  1.6478159e-02, -6.9698272e-03],
       [ 4.2552163e-04,  1.8525815e-02, -1.8951342e-02],
       [ 3.2431120e-04,  1.5905827e-02, -1.6230145e-02]], dtype=float32)>, 
       
<tf.Tensor: id=133560, shape=(3,), dtype=float32, numpy=array([-0.00201432,  0.01235731, -0.010343  ], dtype=float32)>]


    - model.variables: モデルの変数


    [
        <tf.Variable 'dense/kernel:0' shape=(4, 10) dtype=float32, numpy=
array([[-0.3843205 ,  0.43735412,  0.01793659, -0.00581592,  0.25614265,
         0.10672271,  0.17186058, -0.23851329, -0.5058493 , -0.60955423],
       [-0.3938911 ,  0.914218  , -0.47044384, -0.19632381,  0.6941438 ,
         0.30981922, -0.34261385, -0.57393235,  0.07779746, -0.21541847],
       [-0.3775152 , -0.676387  , -0.16800904, -0.28256077,  0.7994718 ,
        -0.20087023,  0.22649263, -0.21446362,  0.5850402 ,  0.8311506 ],
       [-0.33040726, -0.56265694, -0.25879812,  0.47858787,  0.25426   ,
        -0.5926699 ,  0.77422   , -0.45039326, -0.27964562,  0.80207825]],
      dtype=float32)>, 
      
<tf.Variable 'dense/bias:0' shape=(10,) dtype=float32, numpy=
array([ 0.        ,  0.17922321,  0.        ,  0.        ,  0.03234446,
        0.01800845, -0.03559922,  0.        , -0.00631575, -0.09511752],
      dtype=float32)>, 
      
<tf.Variable 'dense_1/kernel:0' shape=(10, 10) dtype=float32, numpy=
array([[ 0.15399748, -0.49363282,  0.03810507,  0.24534875,  0.30468953,
         0.3952753 , -0.5386307 ,  0.36433148,  0.14814144, -0.51871264],
       [-0.38835508, -0.22895482,  1.0606318 , -0.8451014 ,  0.4334192 ,
         0.3068812 , -0.19265601, -0.20900226,  0.37774006,  0.3175367 ],
       [-0.4012698 ,  0.4446566 , -0.44406947, -0.33192012, -0.2906304 ,
        -0.04555786,  0.36753345,  0.15335763, -0.21264213, -0.39329678],
       [ 0.38313067, -0.15171978,  0.36287373,  0.12642491, -0.2621108 ,
         0.4966687 , -0.16316953, -0.01851141, -0.16824704, -0.02655894],
       [-0.21955031, -0.12587565,  0.31622502,  0.7103451 , -0.31813595,
        -0.40880948, -0.20107889, -0.41473347, -0.06297125, -0.5039244 ],
       [-0.10961637,  0.36342466, -0.13737124, -0.12172164, -0.22772269,
         0.25983518,  0.3232332 , -0.18851417,  0.6528024 , -0.1955275 ],
       [-0.14553961,  0.41952837, -0.29009616,  0.70999414, -0.34715295,
         0.03791195,  0.3420205 ,  0.35800284, -0.11809247,  0.26614803],
       [-0.45626032,  0.2787254 , -0.40420106,  0.30429333, -0.07300165,
         0.09530073,  0.18836373, -0.15799135, -0.10399941,  0.53501594],
       [-0.42256474, -0.26309896,  0.438885  ,  0.35526007, -0.3915452 ,
        -0.15181771,  0.15586972,  0.04420429, -0.46951613, -0.32622862],
       [-0.4984072 , -0.50452733, -0.66944396,  0.5378908 , -0.09215099,
        -0.38426495, -0.5183203 ,  0.15354097,  0.65077275, -0.17393005]],
      dtype=float32)>, 
      
<tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=
array([ 0.        ,  0.        ,  0.20420292, -0.01898297, -0.00694644,
        0.        ,  0.        ,  0.        , -0.01028282,  0.        ],
      dtype=float32)>, 
      
<tf.Variable 'dense_2/kernel:0' shape=(10, 3) dtype=float32, numpy=
array([[ 0.29421955,  0.5769217 , -0.28114784],
       [-0.37446254,  0.09097826, -0.07705456],
       [ 0.8676396 ,  0.6099623 , -1.0459243 ],
       [-1.2189142 , -0.15661784,  0.48545402],
       [-0.6130914 , -0.36197677, -0.01680992],
       [ 0.43516195, -0.06205237,  0.19222468],
       [ 0.5410805 , -0.31813917, -0.12938541],
       [ 0.51387346, -0.4228638 , -0.10818648],
       [ 0.28549263, -0.7402089 ,  0.23255937],
       [-0.26558006, -0.3725694 ,  0.43951643]], dtype=float32)>, 
       
<tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32, numpy=array([ 0.2680519 , -0.08609052, -0.18196136], dtype=float32)>]
    """


"""
モデルの評価: (別のラベル付きデータセットを用いて正答率を測ってみる)
"""

test_fp = "/Users/tomokatsuiguchi/.keras/datasets/iris_test.csv"

test_dataset = tf.data.TextLineDataset(test_fp)
test_dataset = test_dataset.skip(1)             # skip header row
test_dataset = test_dataset.map(parse_csv)      # parse each row with the funcition created earlier
test_dataset = test_dataset.shuffle(1000)       # randomize
test_dataset = test_dataset.batch(32)           # use the same batch size as the training set

test_accuracy = tfe.metrics.Accuracy()

for (x, y) in tfe.Iterator(test_dataset):
  prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)
  test_accuracy(prediction, y)

print("Test set accuracy: {:.3%}".format(test_accuracy.result())) # 96.667%

"""
学習済みモデルを使った分類: ラベルなしデータセットで分類
"""

class_ids = ["Iris setosa", "Iris versicolor", "Iris virginica"]

predict_dataset = tf.convert_to_tensor([
    [5.1, 3.3, 1.7, 0.5,],
    [5.9, 3.0, 4.2, 1.5,],
    [6.9, 3.1, 5.4, 2.1]
])

predictions = model(predict_dataset)

for i, logits in enumerate(predictions):
  class_idx = tf.argmax(logits).numpy()
  name = class_ids[class_idx]
  print("Example {} prediction: {}".format(i, name))